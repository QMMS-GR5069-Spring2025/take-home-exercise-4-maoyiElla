{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644bd43f-086d-4b02-8471-db180b26fefa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a72d11c4-a8b4-46ec-b34f-4546ff05f139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up S3 client and bucket\n",
    "s3 = boto3.client('s3')\n",
    "bucket = \"columbia-gr5069-main\"\n",
    "\n",
    "# List of files to load from the bucket\n",
    "keys = {\n",
    "    \"races\": \"raw/races.csv\",\n",
    "    \"results\": \"raw/results.csv\",\n",
    "}\n",
    "\n",
    "# Dictionary to store the loaded DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through and load each CSV into a DataFrame\n",
    "for name, key in keys.items():\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    dataframes[name] = df\n",
    "    print(f\"Loaded {name} ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
    "\n",
    "# Example usage:\n",
    "races_df = dataframes['races']\n",
    "results_df = dataframes['results']\n",
    "\n",
    "# Preview a DataFrame\n",
    "display(races_df)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de1f961-2170-4067-b8b6-04d825e09b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert races_df from pandas to PySpark\n",
    "races_df_spark = spark.createDataFrame(races_df)\n",
    "results_df_spark = spark.createDataFrame(results_df)\n",
    "\n",
    "# Join results_df with races_df_spark on \"raceId\" to get \"year\"\n",
    "data_df = results_df_spark.join(races_df_spark.select(\"raceId\", \"year\"), on=\"raceId\", how=\"inner\")\n",
    "\n",
    "# Select only needed columns\n",
    "data_df = data_df.select(\n",
    "    \"grid\", \"constructorId\", \"driverId\", \"year\", \"position\"\n",
    ")\n",
    "\n",
    "# Filter out rows where position is null or '\\N' (invalid)\n",
    "data_df = data_df.filter(\n",
    "    (col(\"position\").isNotNull()) & (col(\"position\") != '\\\\N')\n",
    ")\n",
    "\n",
    "# Convert \"position\" column to integer\n",
    "data_df = data_df.withColumn(\"position\", col(\"position\").cast(\"int\"))\n",
    "\n",
    "# Show sample\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9fce27-c5da-41ef-9195-784e625c8b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Question 1: Create two (2) new tables in your own fatabse where you'll store the predictions from each model for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae3ac9c-ce8f-4c1b-a15a-4eb0b48ca2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mysql-connector-python\n",
    "import mysql.connector\n",
    "\n",
    "# Connect to database\n",
    "conn = mysql.connector.connect(\n",
    "    host='ml4995-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com',\n",
    "    user='admin',\n",
    "    password='123456789',\n",
    "    database='gr5069'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table for Linear Regression predictions\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS linear_model_preds (\n",
    "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        grid INT,\n",
    "        constructorId INT,\n",
    "        driverId INT,\n",
    "        year INT,\n",
    "        true_position INT,\n",
    "        predicted_position DOUBLE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Create table for Random Forest predictions\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS rf_model_preds (\n",
    "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        grid INT,\n",
    "        constructorId INT,\n",
    "        driverId INT,\n",
    "        year INT,\n",
    "        true_position INT,\n",
    "        predicted_position DOUBLE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Close\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "150b1ee5-d8bc-4699-b3cb-9b19ef813c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Build two (2) predictive models using MLflow, logging hyperparameters, the model itself, four metrics, and two artifcats. Submit submit your MLflow experiments as part of your assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736ae9fd-5656-47ee-a105-f7f724cdd323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for the linear model prediction\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Vector Assembler\n",
    "vec_assembler = VectorAssembler(\n",
    "    inputCols=[\"grid\", \"constructorId\", \"driverId\", \"year\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "vec_data = vec_assembler.transform(data_df).select(\"features\", \"position\", \"grid\", \"constructorId\", \"driverId\", \"year\")\n",
    "\n",
    "# 2. Train/Test Split\n",
    "train_data, test_data = vec_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 3. Initialize model with hyperparameter\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"position\", elasticNetParam=0.3)\n",
    "\n",
    "# 4. MLflow Logging\n",
    "with mlflow.start_run(run_name=\"LinearRegression_F1_Logged\") as run:\n",
    "    \n",
    "    # Fit model\n",
    "    model = lr.fit(train_data)\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluation\n",
    "    evaluator = RegressionEvaluator(labelCol=\"position\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "    mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "    r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "    mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"elasticNetParam\", 0.3)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.spark.log_model(model, \"model\")\n",
    "\n",
    "    # Save predictions as artifact\n",
    "    predictions_full = predictions.select(\"features\", \"prediction\") \\\n",
    "        .join(vec_data, on=\"features\") \\\n",
    "        .select(\"grid\", \"constructorId\", \"driverId\", \"year\", col(\"position\").alias(\"true_position\"), \"prediction\")\n",
    "\n",
    "    pred_pd = predictions_full.toPandas()\n",
    "    pred_pd.to_csv(\"linear_predictions.csv\", index=False)\n",
    "    mlflow.log_artifact(\"linear_predictions.csv\")\n",
    "\n",
    "    # Save residual plot as artifact\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(pred_pd[\"true_position\"], pred_pd[\"prediction\"], alpha=0.6)\n",
    "    plt.xlabel(\"True Position\")\n",
    "    plt.ylabel(\"Predicted Position\")\n",
    "    plt.title(\"Linear Regression: True vs Predicted\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"residual_plot.png\")\n",
    "    mlflow.log_artifact(\"residual_plot.png\")\n",
    "\n",
    "    print(f\"✅ Logged to MLflow. RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.2f}, MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365106b4-3db9-4044-9b88-062e06b7c031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for the random forest model prediction (the second model)\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Vector Assembler (re-use vec_assembler if needed)\n",
    "vec_assembler_rf = VectorAssembler(\n",
    "    inputCols=[\"grid\", \"constructorId\", \"driverId\", \"year\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "vec_data_rf = vec_assembler_rf.transform(data_df).select(\"features\", \"position\", \"grid\", \"constructorId\", \"driverId\", \"year\")\n",
    "\n",
    "# 2. Train/Test Split\n",
    "train_data_rf, test_data_rf = vec_data_rf.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 3. Initialize Random Forest\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"position\", numTrees=100, maxDepth=5)\n",
    "\n",
    "# 4. MLflow Logging\n",
    "with mlflow.start_run(run_name=\"RandomForest_F1\") as run:\n",
    "\n",
    "    # Fit model\n",
    "    rf_model = rf.fit(train_data_rf)\n",
    "\n",
    "    # Predict\n",
    "    predictions_rf = rf_model.transform(test_data_rf)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluator_rf = RegressionEvaluator(labelCol=\"position\", predictionCol=\"prediction\")\n",
    "    rmse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"rmse\"})\n",
    "    mae_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mae\"})\n",
    "    r2_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"r2\"})\n",
    "    mse_rf = evaluator_rf.evaluate(predictions_rf, {evaluator_rf.metricName: \"mse\"})\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"numTrees\", 100)\n",
    "    mlflow.log_param(\"maxDepth\", 5)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse_rf)\n",
    "    mlflow.log_metric(\"mae\", mae_rf)\n",
    "    mlflow.log_metric(\"r2\", r2_rf)\n",
    "    mlflow.log_metric(\"mse\", mse_rf)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.spark.log_model(rf_model, \"model\")\n",
    "\n",
    "    # Save predictions as artifact\n",
    "    predictions_rf_full = predictions_rf.select(\"features\", \"prediction\") \\\n",
    "        .join(vec_data_rf, on=\"features\") \\\n",
    "        .select(\"grid\", \"constructorId\", \"driverId\", \"year\", col(\"position\").alias(\"true_position\"), \"prediction\")\n",
    "\n",
    "    pred_rf_pd = predictions_rf_full.toPandas()\n",
    "    pred_rf_pd.to_csv(\"rf_predictions.csv\", index=False)\n",
    "    mlflow.log_artifact(\"rf_predictions.csv\")\n",
    "\n",
    "    # Save residual plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(pred_rf_pd[\"true_position\"], pred_rf_pd[\"prediction\"], alpha=0.6)\n",
    "    plt.xlabel(\"True Position\")\n",
    "    plt.ylabel(\"Predicted Position\")\n",
    "    plt.title(\"Random Forest: True vs Predicted\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"rf_residual_plot.png\")\n",
    "    mlflow.log_artifact(\"rf_residual_plot.png\")\n",
    "\n",
    "    print(f\"✅ Random Forest Logged! RMSE: {rmse_rf:.2f}, MAE: {mae_rf:.2f}, R²: {r2_rf:.2f}, MSE: {mse_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63eaa9a5-7c71-40ea-8ac3-b68d47262147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. For each model, store its predictions in the corresponding table you created in your own database. Ensure you are using your own database to store your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1cd3b38-8baa-4166-ba72-80721abd1124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save predictions from Linear Regression model to MySQL\n",
    "predictions_full.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://ml4995-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"linear_model_preds\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"123456789\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d9cdf8-c59d-404c-85d8-9be839fd95cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save predictions from Random Forest model to MySQL\n",
    "predictions_rf_full.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://ml4995-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"rf_model_preds\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"123456789\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e88653-c7c3-4f77-a3b9-72895b7f5163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Homework #5: model deployment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
